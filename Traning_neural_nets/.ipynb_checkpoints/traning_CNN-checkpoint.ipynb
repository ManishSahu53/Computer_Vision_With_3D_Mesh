{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datetime import timedelta\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.contrib.layers import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels=[]\n",
    "images=[]\n",
    "for i in range(10000):\n",
    "    file_name = \"../data_set/test_images1/anim{}.png\".format(i)\n",
    "    img = cv2.imread(file_name)\n",
    "    images.append(img.tolist())\n",
    "    labels.append(1)\n",
    "for i in range(10000):\n",
    "    file_name = \"../data_set/test_images2/anim{}.png\".format(i)\n",
    "    img = cv2.imread(file_name)\n",
    "    images.append(img.tolist())\n",
    "    labels.append(2)\n",
    "for i in range(5000):\n",
    "    file_name = \"../data_set/test_images3/anim{}.png\".format(i)\n",
    "    img = cv2.imread(file_name)\n",
    "    images.append(img.tolist())\n",
    "    labels.append(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 307, 400, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "X_train,y_train=shuffle(images,labels)\n",
    "\n",
    "\n",
    "\n",
    "mean_u=0\n",
    "sigma=0.1\n",
    "\n",
    "#layer 1 whcihsaver = tf.train.Saver is 32*32*3 as input and 28*28*12 as output\n",
    "conv1_w=tf.Variable(tf.truncated_normal(shape=(5,5,3,12),mean=mean_u,stddev=sigma))\n",
    "conv1_b=tf.Variable(tf.zeros(12))\n",
    "\n",
    "#layer2 which is 14*14*12 as input, and the output is 10*10*16 as output\n",
    "conv2_w=tf.Variable(tf.truncated_normal(shape=(5,5,12,16),mean=mean_u,stddev=sigma))\n",
    "conv2_b=tf.Variable(tf.zeros(16))\n",
    "\n",
    "#layer 3:\n",
    "fun1_w=tf.Variable(tf.truncated_normal(shape=(400,120),mean=mean_u,stddev=sigma))\n",
    "fun1_b=tf.Variable(tf.zeros(120))\n",
    "\n",
    "    \n",
    "#layer 4\n",
    "fun2_w=tf.Variable(tf.truncated_normal(shape=(120,84),mean=mean_u,stddev=sigma))\n",
    "fun2_b=tf.Variable(tf.zeros(84))\n",
    "\n",
    "#layrer 5\n",
    "fun3_w=tf.Variable(tf.truncated_normal(shape=(84,43),mean=mean_u,stddev=sigma))\n",
    "fun3_b=tf.Variable(tf.zeros(43))\n",
    "\n",
    "def Neuralnets(X):\n",
    "    #layer 1 whcih is 32*32*3 as input and 28*28*12 as output\n",
    "    conv1=tf.nn.conv2d(X,conv1_w,strides=[1,1,1,1],padding='VALID')+conv1_b\n",
    "    \n",
    "    #apply activation:\n",
    "    conv1=tf.nn.relu(conv1)\n",
    "    \n",
    "    #apply max_pooling: output is 14*14*12\n",
    "    conv1=tf.nn.max_pool(conv1,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
    "    \n",
    "    #layer2 which is 14*14*12 as input, and the output is 10*10*16 as output\n",
    "    conv2=tf.nn.conv2d(conv1,conv2_w,strides=[1,1,1,1],padding='VALID')+conv2_b\n",
    "    conv2=tf.nn.relu(conv2)\n",
    "    #apply max pooling and we get 5*5*16 output\n",
    "    conv2=tf.nn.max_pool(conv2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='VALID')\n",
    "    \n",
    "    #flatten we get 400\n",
    "    fun0=flatten(conv2)\n",
    "    \n",
    "    #layer 3:\n",
    "    fun1=tf.matmul(fun0,fun1_w)+fun1_b\n",
    "    \n",
    "    #apply actiovation: output 120\n",
    "    fun1=tf.nn.relu(fun1)\n",
    "    \n",
    "    #layer 4\n",
    "    fun2=tf.matmul(fun1,fun2_w)+fun2_b\n",
    "    \n",
    "    #apply actiovation:\n",
    "    fun2=tf.nn.relu(fun2)\n",
    "    \n",
    "    logits=tf.matmul(fun2,fun3_w)+fun3_b\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#construct functions for the neural nets\n",
    "x=tf.placeholder(tf.float32,(None,32,32,3))\n",
    "y=tf.placeholder(tf.int32,(None))\n",
    "one_hot_y=tf.one_hot(y,3)\n",
    "logits=Neuralnets(x)\n",
    "cross_entropy=tf.nn.softmax_cross_entropy_with_logits(logits,one_hot_y)\n",
    "loss_operation=tf.reduce_mean(cross_entropy)\n",
    "optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_operation=optimizer.minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cross_prediction=tf.equal(tf.argmax(logits,1),tf.argmax(one_hot_y,1))\n",
    "accuracy_operation=tf.reduce_mean(tf.cast(cross_prediction,tf.float32))\n",
    "\n",
    "def evaluation(X_data,y_data):\n",
    "    num_examples=len(X_data)\n",
    "    total_accuracy=0\n",
    "    sess=tf.get_default_session()\n",
    "    for offset in range(0,num_examples,batch_size):\n",
    "        batch_x,batch_y=X_data[offset:offset+batch_size],y_data[offset:offset+batch_size]\n",
    "        accuracy=sess.run(accuracy_operation,feed_dict={x:batch_x,y:batch_y})\n",
    "        total_accuracy+=(accuracy*len(batch_x))\n",
    "    return total_accuracy/num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data,validation_data,training_label,validation_label=train_test_split(X_train,y_train,test_size=0.2)\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples=len(training_data)\n",
    "    print(\"we are training our model\")\n",
    "    print()\n",
    "    for i in range(training_epochs):\n",
    "        X_train1,y_train1=shuffle(training_data,training_label)\n",
    "        for offset in range(0,num_examples,batch_size):\n",
    "            end=offset+batch_size\n",
    "            batch_x,batch_y=X_train[offset:end],y_train[offset:end]\n",
    "            sess.run(training_operation,feed_dict={x:batch_x,y:batch_y})\n",
    "        v_accuracy=evaluation(validation_data,validation_label)\n",
    "        print(\"epoch{}:\".format(i+1))\n",
    "        print(\"the validation accuracy:{:.3f}\".format(v_accuracy))\n",
    "        print()\n",
    "    cross=tf.equal(tf.argmax(logits,1),tf.argmax(one_hot_y,1))\n",
    "    accuracy=tf.reduce_mean(tf.cast(cross_prediction,tf.float32))   \n",
    "    print(\"the test accuracy after using regularization is:\",accuracy.eval({x:X_test,y:y_test}))\n",
    "    #saver.save(sess, save_file)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
